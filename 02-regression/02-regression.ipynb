{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a15b50",
   "metadata": {},
   "source": [
    "## 2. Machine Learning for Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98763427",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4c9d0",
   "metadata": {},
   "source": [
    "## 2.2 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea79f27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-02-car-price/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94aca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget $data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ab6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()  ##Look for header inconsistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83869b-6c7f-4e30-a80a-baa00adbcaf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleaning header (make it consistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d285bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "df.columns  #.columns is a data structure of pandas, similar to series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b75b4-6b5f-4aa0-8a75-55bae5bf1102",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleaning values, let's take 'make' column as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22165705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['make'].str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7018c-d81c-40d5-a4e6-b34b68b388e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### However, we need to apply this to all the column values, not only for make column. First detect the string column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afccb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes  ##List column's value type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b38f24-e180-4e75-a4ec-02199bd17e35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get only string types, which is object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f01a63-c675-4627-a086-19e230c1693d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes == 'object'\n",
    "df.dtypes[df.dtypes == 'object']  ## Get only object type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c9ce1-89da-4bf8-820f-ec45e6acdc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "strings = list(df.dtypes[df.dtypes == 'object'].index) ## Get the index of the series and convert it to list \n",
    "strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a183602-8699-4f0e-862f-544323ba27bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loop through all string columns AND clean the columns value, just like we did for make column earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ccdbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in strings:\n",
    "    df[col] = df[col].str.lower().str.replace(' ', '_')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55b77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f691b6",
   "metadata": {},
   "source": [
    "## 2.3 Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d7b9f-8573-4a40-b887-0e9a3a3bc38c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Exploratory data analysis (EDA) is an essential step in the data analysis process. It involves summarizing and visualizing the main characteristics of a dataset to gain insights and identify patterns or trends. By summarizing, visualizing, and cleaning the data, researchers can uncover patterns, identify relationships, and make informed decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158d36c-0ec3-4e67-9761-2d99c999cee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Before doing EDA, let's look at each column and print some values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e350fe-64da-4cd1-88b4-2b63f1bd76fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)\n",
    "    print(df[col].head())\n",
    "    print()  #new line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9e92d-d297-4f7a-97fe-4d63ded1f307",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's find unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863509c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    # print(col)\n",
    "    # print(df[col].unique()[:5]) # print only first five unique value\n",
    "    print(f\"number of unique values in {col} column: {df[col].nunique()}\")  #notice the nunique vs unique. n means count\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acc47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea970a9",
   "metadata": {},
   "source": [
    "### Distribution of price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932ed3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install seaborn matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Display plot in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9312d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(df.msrp, bins=50)   #bins number of bars in histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559be5ec-3908-4204-95da-2ba9aeaa87b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "From graph, we see that lots of cars are cheap and only few are expensive. That means it is LONG-TAIL Distribution (many prices in a small range, but a few prices in a wide range) we need to zoom in a bit to ignore the long tail with too less datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcff5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(df.msrp[df.msrp < 100000], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20402058-6bab-4186-bce4-087174820cb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "This kind of distribution (long tail, and the peak) is not good for ML models, because this distribution will confuse them. There is a way to get rid of the long tail, by applying logarithm (it compresses large values while spreading out smaller ones) to the price. This results in more compact values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7747e2-ccdb-4418-9f83-1fcd5a652d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#np.log([0, 1,10,1000,100000])\n",
    "# problem with logarithm is when we have a 0, because log(0) does not exist\n",
    "#np.log([0 + 1, 1 + 1, 10 + 1, 1000 + 1, 100000 + 1])\n",
    "# Output: array([ 0.        ,  0.69314718,  2.39789527,  6.90875478, 11.51293546])\n",
    "# \n",
    "# In order to not always add 1 there is a NumPy function\n",
    "#np.log1p([0, 1,10,1000,100000])\n",
    "# Output: array([ 0.        ,  0.69314718,  2.39789527,  6.90875478, 11.51293546])\n",
    "\n",
    "price_logs = np.log1p(df.msrp)\n",
    "price_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272380ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(price_logs, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccdcdcb-02ad-4d44-8c29-d4820326051f",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can see the long tail is gone and you see a nice bell curve shape of a so called normal distribution, what is ideal for ML models. But still there is the strange peak. This could be the minimum price of $1,000 of the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea903334",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5746f13-761a-480e-a4f2-bd2fab58c6f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "The sum function sums across columns and shows for each column how much missing values (Nan) are there. This information is important when training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb51a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7841c65",
   "metadata": {},
   "source": [
    "## 2.4 Setting up the validation framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f42c40",
   "metadata": {},
   "source": [
    "Let's draw it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438e1a5-37d0-4217-a7f8-e9c135da3eed",
   "metadata": {
    "tags": []
   },
   "source": [
    "To validate the model, we take the dataset and split it into three parts (train-val-test / 60-20-20). \n",
    "This means that we train the model on the training dataset, check if it works fine on the validation dataset, and leave the test dataset for the end.\n",
    "For each of these three parts, we create the feature matrix X and the target variable y (Xtrain, ytrain, Xval, yval, Xtest, ytest). \n",
    "So, what we need to do is calculate how much 20% is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e222f-2904-45af-93a9-21ccf67a7558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df) # Number of records of the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77be83-c755-4eaf-b502-73b81623833b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "int(len(df) * 0.2) #Calculate 20% of whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6a8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Splitting data set intto three \n",
    "\n",
    "n = len(df)\n",
    "n_val = int(n * 0.2)\n",
    "n_test = int(n * 0.2)\n",
    "n_train = n - n_val - n_test\n",
    "\n",
    "n, n_val+n_test+n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075ac25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_val, n_test, n_train #sizes of our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcfb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.iloc[[10, 0, 3, 5]]  #iloc used to select row number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d49f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dataset has been split into three\n",
    "df_train = df.iloc[:n_train]  #get data upto 7149\n",
    "df_val = df.iloc[n_train:n_train+n_val] #get data from 7150 to 9531\n",
    "df_test = df.iloc[n_train+n_val:] #get data from 9532 to rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42048c-2d63-41de-bd99-12a531c6c40e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e00956-8b77-40a2-a934-21966db5bf29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc64bdb-3396-4120-9ffd-0bfab2788fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7fa5d-bcbc-465d-afe2-97c3b779cd4f",
   "metadata": {},
   "source": [
    "After the divisions, there is one crucial problem, the sequential problem. That’s a problem when there is an order in the dataset. \n",
    "That means we need to shuffle, otherwise, there are BMWs only in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7960f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = np.arange(n)\n",
    "idx #See, the output is sequential, we need to shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45539a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's shuffle. To make it reproducible, we may use seed\n",
    "# np.random.seed(2)\n",
    "np.random.shuffle(idx)\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3ce48d-6cab-42cd-850c-9b0276513908",
   "metadata": {},
   "source": [
    "Using this shuffled index we can create our shuffled datasets for training, validation and for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3180c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df.iloc[idx[:n_train]]  #select multiple columns from idx. It selects shuffled columns index for all rows in the DataFrame.\n",
    "df_val = df.iloc[idx[n_train:n_train+n_val]]\n",
    "df_test = df.iloc[idx[n_train+n_val:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d31d1-c382-49c7-a373-d7dd874c048b",
   "metadata": {},
   "source": [
    "Now there is no order in the index column so we can reset index and drop the old index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00cbf4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85713c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300cebae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109bc4d7-cb3e-4bbe-a98a-8c05a142fe68",
   "metadata": {
    "tags": []
   },
   "source": [
    "Remember that we should apply the log1p transformation to the price column to help the model perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc394a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = np.log1p(df_train.msrp.values)\n",
    "y_val = np.log1p(df_val.msrp.values)\n",
    "y_test = np.log1p(df_test.msrp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b8a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389c4c6-386f-422e-b4a6-138e7c5b33cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "We should remove msrp values from dataframes (df_train, df_val, df_test) to make sure that, we don’t accidentally use it for training purposes. Since it has long tail distribution which will confuses the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca205d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del df_train['msrp']\n",
    "del df_val['msrp']\n",
    "del df_test['msrp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82fc73",
   "metadata": {},
   "source": [
    "## 2.5 Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332cb0f-c5b1-4d58-8577-db779735a778",
   "metadata": {},
   "source": [
    "Source: https://knowmledge.com/2023/09/20/ml-zoomcamp-2023-machine-learning-for-regression-part-4/\n",
    "\n",
    "Explanation: https://chatgpt.com/share/67dcf911-8848-8012-b97e-2a72ac30eab3\n",
    "\n",
    "Linear regression is a way to predict numbers based on some input information. For example, if we want to predict the **price of a car**, we can use details like **its age, mileage, and horsepower**.  \n",
    "\n",
    "### The General Idea:  \n",
    "- **X (Feature Matrix)** → All the details about many cars (like a table with rows and columns).  \n",
    "- **y (Target)** → The price of each car.  \n",
    "- **g (Model)** → The linear regression formula that learns the relationship between features and price.  \n",
    "\n",
    "### Looking at One Car:  \n",
    "Instead of looking at many cars, let’s focus on just **one car**.  \n",
    "- **xi** → The details of that one car (e.g., age, mileage, horsepower).  \n",
    "- **yi** → The price of that car.  \n",
    "- **g(xi)** → The model’s prediction for that car’s price.  \n",
    "\n",
    "So, the model takes **xi (car details)** and predicts **yi (price)** using a formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf38673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[9]  #Just taking one car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbfe46-1043-4ce1-acf0-0154ae91ac37",
   "metadata": {},
   "source": [
    "We take as an example the characteristic enging_hp, city_mpg, and popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04e7d5-5868-4756-a5dc-56df9fee9c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xi = [180, 11, 1851]  #Old value\n",
    "xi = [230, 18, 3916]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc4706-8830-4e84-82ef-a10432b18fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "That’s almost everything we need to implement g(xi) ~ yi:\n",
    "\n",
    "xi = (180, 11, 1851) with i = 10\n",
    "\n",
    "need to implement the function g(xi2,xi2, … , xin) ~ yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45d20b-6685-4dda-a657-9d2bfc25b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in code this would look like --> this is what we want to implement\n",
    " \n",
    "def g(xi):\n",
    "    # do something and return the predicted price\n",
    "    return 10000\n",
    " \n",
    "g(xi)\n",
    "# Output: 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c1ab3-a944-4d6f-8ebf-06061ab82639",
   "metadata": {
    "tags": []
   },
   "source": [
    "This function g is still not very useful, because it always returns a fixed price. \n",
    "\n",
    "We need to implement the function g(xi) = w0 + w1xi1 + w2xi2 + w3xi3 with w0 as bias term and w1, w2, and w3 as weights. This formula can be written as\n",
    "\n",
    "https://knowmledge.com/wp-content/uploads/2023/09/linregex-1.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a99de-f9a3-45b3-b7c5-320785813c14",
   "metadata": {},
   "source": [
    "However, the formula of linear regression in python looks like this since index start from 0 instead of 1\n",
    "https://knowmledge.com/wp-content/uploads/2023/09/linreggen-1.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0ddfe-ed2e-40c0-9e2d-18c707b50b1b",
   "metadata": {},
   "source": [
    "The following snippet shows the implementation of the g-function (renamed as linear_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7423e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(xi):\n",
    "    n = len(xi)\n",
    "\n",
    "    pred = w0\n",
    "\n",
    "    for j in range(n):\n",
    "        pred = pred + w[j] * xi[j]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample values for w0 and w and the given xi\n",
    "xi = [230, 18, 3916]\n",
    "w0 = 0\n",
    "w = [1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec717e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9def6-5bb8-43a7-8992-93d9c4c4dd9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try some other values\n",
    "w0 = 7.17\n",
    "w = [0.01, 0.04, 0.002]\n",
    "linear_regression(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9df3bf-3b8e-4173-bf16-75b1ee295bb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What does this actually mean?\n",
    "\n",
    "We’ve just implemented the formula as mentioned before with given values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce13c9c-df32-4b88-af22-8320d6bea30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.17 + 230*0.01 + 18*0.04 + 3916*0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182fd50-c69e-4a90-9be7-faf00fdeb0ed",
   "metadata": {},
   "source": [
    "- w0 = 7.17 bias term = the prediction of a car, if we don’t know anything about this\n",
    "- engine_hp: 230 * 0.01 that means in this case per 100 hp the price will increase by $1\n",
    "- city_mpg: 18 * 0.04 that means analog to hp, the more gallons the higher the price will be\n",
    "- popularity: 3916 * 0.002 analog, but it doesn’t seem that it’s affecting the price too much, so for every extra mention on twitter the car becomes just a little bit more expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa494e2d-9792-41bc-82a1-24731c9f0b9a",
   "metadata": {},
   "source": [
    "There is still one important step to do. Because we logarithmized (log(y+1)) the price at the beginning, we now have to undo that. This gives us the predicted price in $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1b9a2-138c-41c9-863e-2536fe78301c",
   "metadata": {},
   "source": [
    "#### Get the real prediction for the price in $\n",
    "We do \"-1\" here to undo the \"+1\" inside the log. Shortcut for -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d824e-506c-4024-b9be-315382d8712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expm1(18.022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a59047-aca8-47dd-a9ed-def5659facb7",
   "metadata": {},
   "source": [
    "Just for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788e157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.log1p(67120494.33915682)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4a410",
   "metadata": {},
   "source": [
    "## 2.6 Linear regression vector form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aef4f3-bd15-4519-8c28-6367f247f33c",
   "metadata": {},
   "source": [
    "Source: https://knowmledge.com/2023/09/20/ml-zoomcamp-2023-machine-learning-for-regression-part-5/\n",
    "\n",
    "Now we will generalize to a vector form of what we did in last. \n",
    "\n",
    "That means coming back from only one observation xi (of one car) to the whole feature matrix X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f9279-de58-4b49-bfe7-4ccdcd53d506",
   "metadata": {},
   "source": [
    "https://knowmledge.com/wp-content/uploads/2023/09/linreggen-1.jpg\n",
    "\n",
    "Looking at the last part of this formula we see the dot product (vector-vector multiplication).\n",
    "\n",
    "g(xi) = w0 + xiTw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s implement again the dot product (vector-vector multiplication)\n",
    "def dot(xi, w):\n",
    "    n = len(xi)\n",
    "    \n",
    "    res = 0.0\n",
    "    \n",
    "    for j in range(n):\n",
    "        res = res + xi[j] * w[j]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8609d-5d1e-4a15-96b1-859f585b25c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Based on that the implementation of the linear_regression function could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92386229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(xi):\n",
    "    return w0 + dot(xi, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9ef72-adc9-4e41-8425-428f887d0278",
   "metadata": {
    "tags": []
   },
   "source": [
    "To make the last equation more simple, we can imagine there is one more feature xi0, that is always equal to 1.\n",
    "\n",
    "#g(xi) = w0 + xiTw -> g(xi) = w0xi0 + xiTw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62021819-40fe-4aac-b442-e91eb6df8bd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "That means we can use the dot product for the entire regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b060c-f9c6-4f08-a1e6-2ada77195e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xi = [180, 11, 1851]  #old value\n",
    "xi = [230, 18, 3916]\n",
    "w0 = 7.17\n",
    "w = [0.01, 0.04, 0.002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding w0 to the vector w\n",
    "w_new = [w0] + w\n",
    "w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0f2d6-f79f-4fb7-8299-00d3470002df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2f475-8648-4179-82d6-e481a3694886",
   "metadata": {
    "tags": []
   },
   "source": [
    "The updated code for linear_regression function looks now like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822ffd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(xi):\n",
    "    xi = [1] + xi\n",
    "    return dot(xi, w_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ae870",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression(xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f0513-e7a0-4513-8ae6-57331d8edf59",
   "metadata": {},
   "source": [
    "let’s go back to thinking about all the examples together. \n",
    "\n",
    "X is a m x (n+1) dimensional matrix (with m rows and n+1 columns)\n",
    "\n",
    "https://knowmledge.com/wp-content/uploads/2023/09/xandw.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c8ec0-20c7-41c7-ab65-d1e062a75a51",
   "metadata": {},
   "source": [
    "What we have to do here, for each row of X we multiply this row with the vector w. \n",
    "\n",
    "This vector contains our predictions, therefor we call it ypred.\n",
    "\n",
    "https://knowmledge.com/wp-content/uploads/2023/09/ypred.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To sum up. What we need to do to get our model g is a matrix vector multiplication between X and w.\n",
    "w0 = 7.17\n",
    "w = [0.01, 0.04, 0.002]\n",
    "w_new = [w0] + w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1  = [1, 148, 24, 1385]   #assume those are feature of bmw\n",
    "x2  = [1, 132, 25, 2031]   #assume those are feature of toyota\n",
    "x10 = [1, 453, 11, 86]     #assume those are feature of nissan\n",
    "\n",
    "X = [x1, x2, x10]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2e12d-8c09-42af-a2f2-7798f3305de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This turns the list of lists into a matrix\n",
    "X = np.array(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de649ce-889b-4898-9613-6d154468a9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we have predictions, so for each car we have a price for this car\n",
    "y = X.dot(w_new)\n",
    " \n",
    "# shortcut to not do -1 manually to get the real $ price\n",
    "np.expm1(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf22250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet shows the implementation of the adapted linear_regression function\n",
    "def linear_regression(X):\n",
    "    return X.dot(w_new) #Maybe you wonder where the w_new vector comes from, we will talk it about in 2.7 section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear_regression(X)\n",
    "np.expm1(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab2c67",
   "metadata": {},
   "source": [
    "## 2.7 Training a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640608c2-e593-43b6-bf19-b52eb21d63cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Source: https://knowmledge.com/2023/09/21/ml-zoomcamp-2023-machine-learning-for-regression-part-6/\n",
    "\n",
    "\n",
    "Simplify: https://chatgpt.com/share/67de481e-8728-8012-b196-18193a21d190"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4da52e-886a-4765-aba7-bbcd22436bec",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now our goal is to compute the weight vector 𝑤 such that the prediction 𝑋𝑤 closely approximates 𝑦 in linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea79aa-b06f-49ed-a2b1-46eb3a7f936e",
   "metadata": {},
   "source": [
    "However, directly solving Xw=y using w=X −1 y is not feasible because X −1 only exists for square matrices. However, Xis usually m×(n+1), which is not square."
   ]
  },
  {
   "cell_type": "raw",
   "id": "47be17b2-20c1-40c8-bf6d-98bbfcb80fc9",
   "metadata": {},
   "source": [
    "We need to implement the function train_linear_regression, that takes the feature matrix X and the target variable y and returns the vector w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee753ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To approach this implementation we first use a simplified example.\n",
    "X = [      \n",
    "    [148, 24, 1385],\n",
    "    [132, 25, 2031],\n",
    "    [453, 11, 86],\n",
    "    [158, 24, 185],\n",
    "    [172, 25, 201],\n",
    "    [413, 11, 86],\n",
    "    [38,  54, 185],\n",
    "    [142, 25, 431],\n",
    "    [453, 31, 86],\n",
    "]\n",
    "\n",
    "X = np.array(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083227b0-445a-4a04-b4d1-3365e2c8caae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to add a new column with ones to the feature matrix X. That is for the multiplication with w0\n",
    "ones = np.ones(9) #Creates a 1D array with 9 elements, and all the elements are 1\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f8b0b-194b-4127-8b4f-aa8ff0eb9d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X.shape[0] looks at the number of rows and creates the vector of ones\n",
    "ones = np.ones(X.shape[0]) # Example: If X has 5 rows, it creates [1. 1. 1. 1. 1.].\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890fdc6b-a259-4410-ae6c-d0a6f49a9cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we need to stack this vector of ones with our feature matrix X\n",
    "np.column_stack([ones, ones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26864ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([ones, X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [10000, 20000, 15000, 20050, 10000, 20000, 15000, 25000, 12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f213881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAM MATRIX\n",
    "XTX = X.T.dot(X)\n",
    "# Inverse GRAM MATRIX\n",
    "XTX_inv = np.linalg.inv(XTX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfbbb8b-cfae-4228-8502-053bb0ae137f",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following code snippet we test whether the multiplication of XTX with XTX_inv actually produces the Identity matrix I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd51c78-bc1f-468a-83c5-d4d9144f429b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Without round(1) it's not exactly identity matrix but the other values \n",
    "# are very close to 0 --> we can treat them as 0 and take it as identity matrix\n",
    "XTX.dot(XTX_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf218f6-02a2-484d-acb3-ffc23e30d507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This gives us the I matrix\n",
    "XTX.dot(XTX_inv).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa99e8-1e6b-4465-a70a-18dcc1a0f895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now we can implement the formula to get the full w vector.\n",
    "w_full = XTX_inv.dot(X.T).dot(y)\n",
    "w_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d17c1-83c7-4a1f-b621-480364602662",
   "metadata": {},
   "source": [
    "From that vector w_full we can extract w0 and all the other weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = w_full[0]\n",
    "w = w_full[1:]\n",
    "w0, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can implement the function train_linear_regression, \n",
    "#that takes the feature matrix X and the target variable y and returns w0 and the vector w.\n",
    "def train_linear_regression(X, y):\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "\n",
    "    XTX = X.T.dot(X)\n",
    "    XTX_inv = np.linalg.inv(XTX)\n",
    "    w_full = XTX_inv.dot(X.T).dot(y)\n",
    "    \n",
    "    return w_full[0], w_full[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec101dc-c5e6-473c-b043-79c62d130b55",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let’s test this newly implemented function with some simple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd1cbf-8f92-4891-a3b3-16d73f06a661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X =[\n",
    "    [148, 24, 1385],\n",
    "    [132, 25, 2031],\n",
    "    [453, 11, 86],\n",
    "    [158, 24, 185],\n",
    "    [172, 25, 201],\n",
    "    [413, 11, 83],\n",
    "    [38, 54, 185],\n",
    "    [142, 25, 431],\n",
    "    [453, 31, 86],  \n",
    "]\n",
    " \n",
    "X = np.array(X)\n",
    "y = [10000, 20000, 15000, 25000, 10000, 20000, 15000, 25000, 12000]\n",
    " \n",
    "train_linear_regression(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40deb4fc",
   "metadata": {},
   "source": [
    "## 2.8 Car price baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507790e7-224c-49e9-950d-70ace462b97a",
   "metadata": {},
   "source": [
    "Source: https://knowmledge.com/2023/09/21/ml-zoomcamp-2023-machine-learning-for-regression-part-7/\n",
    "\n",
    "Here we’ll use the implemented code from the last steps to build the model.\n",
    "First we start with a simple model while we’re using only numerical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8505eed9-5612-49c4-942b-196fc21df1db",
   "metadata": {},
   "source": [
    "The next code snippet shows how to extract all numerical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eddee5e-90fe-4761-8de6-6e44aec58837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f139931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267566a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We choose the columns engine_hp, engine_cylinders, highway_mpg, city_mpg, and popularity for our base model.\n",
    "base = ['engine_hp', 'engine_cylinders', 'highway_mpg',\n",
    "        'city_mpg', 'popularity']\n",
    "\n",
    "df_train[base].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f8e46-f258-4704-b8f3-faa5c53ca673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We need to extract the values to use them in training.\n",
    "X_train = df_train[base].fillna(0).values\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae1370-70d3-44fa-9d07-5a54855c159d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###Missing values are generally not good for our model. \n",
    "#Therefore, you should always check whether such values are present.\n",
    "df_train[base].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe25cd-d4c2-42d3-9f9f-fe658308d776",
   "metadata": {
    "tags": []
   },
   "source": [
    "As you can see there are two columns (engine_hp & engine_cylinders) that have missing values. \n",
    "The easiest thing we can do is fill them with zeros. \n",
    "But notice filling it with 0 makes the model ignore this feature, because:\n",
    "\n",
    "g(xi) = w0 + xi1w1 + xi2w2\n",
    "\n",
    "if xi1 = 0 then the last equation simplifies to\n",
    "\n",
    "g(xi) = w0 + 0 + xi2w2\n",
    "\n",
    "But 0 is not always the best way to deal with missing values, because that means there is an observation of a car with 0 cylinders or 0 horse powers. \n",
    "And a car without cylinders or 0 horse powers does not make much sense at this point. \n",
    "For the current example this procedure is sufficient for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7aad78-76da-4d26-bfd5-0550e7376692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[base].fillna(0).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50246c1b-4eca-4039-aa5f-dd3c930fc78a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# However, now we need to apply this change in the DataFrame.\n",
    "X_train = df_train[base].fillna(0).values\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f09b2-cfa5-48d1-a8e2-b372e928700e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94756e59-75a9-4917-96c6-c95634099cb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we can train our model using the train_linear_regression function that we’ve implemented in the last article. \n",
    "The function return the value for w0 and and array for vector w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b4d6c-b875-44eb-b1a0-ab062a378d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w0, w = train_linear_regression(X_train, y_train)\n",
    "w0, w\n",
    "# y_pred = w0 + X_train.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594caf7-414c-4f00-817c-0625e618900b",
   "metadata": {},
   "source": [
    "We can use this two variables to apply the model to our training dataset to see how well the model has learned the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w0 + X_train.dot(w)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### next snippet shows how to output these two lists accordingly.\n",
    "# alpha changes the transparency of the bars\n",
    "# bins specifies the number of bars\n",
    "sns.histplot(y_pred, color='red', alpha=0.5, bins=50)\n",
    "sns.histplot(y_train, color='blue', alpha=0.5, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddf7e87f-caa5-4567-ac4f-d55f7fa4515b",
   "metadata": {},
   "source": [
    "You see from this plot that the model is not ideal (blue is not aligned with red line) but it’s better to have an objective way of saying that the model is good or not good. When we start improving the model, we also want to ensure that we really improving it. The next article is about an objective way to evaluate the performance of a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6f4e4",
   "metadata": {},
   "source": [
    "## 2.9 RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8dd186-4d8d-44cb-94c9-84f779f26501",
   "metadata": {},
   "source": [
    "Source: https://knowmledge.com/2023/09/22/ml-zoomcamp-2023-machine-learning-for-regression-part-8/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27cca9cb-c7c8-4cf9-9cbc-3beb5ab4e3a7",
   "metadata": {},
   "source": [
    "RMSE is used to evaluate our model on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf63d1-4661-444d-844b-f47af52b992e",
   "metadata": {},
   "source": [
    "What we did here? Explanation is here: https://knowmledge.com/2023/09/22/ml-zoomcamp-2023-machine-learning-for-regression-part-8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    se = (y - y_pred) ** 2\n",
    "    mse = se.mean()\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c58ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last article we used Seaborn to visualize the performance but now we have an objective metric for the evaluation.\n",
    "rmse(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24c520",
   "metadata": {},
   "source": [
    "## 2.10 Validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a94aaf-12eb-4ac1-9af4-76339efd55fe",
   "metadata": {},
   "source": [
    "Since we don’t know how well the model can apply the learned knowledge to unseen data. So what we want to do now after training the model g on our training dataset, we want to apply it on the validation dataset to see how it performs on unseen data. We use RMSE for validating the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e2e94-772c-411b-980c-b3c10a521826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']\n",
    " \n",
    "X_train = df_train[base].fillna(0).values\n",
    " \n",
    "w0, w = train_linear_regression(X_train, y_train)\n",
    " \n",
    "y_pred = w0 + X_train.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dab153-b720-4f29-90e9-51d21c8ccf46",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we implement the prepare_X function. The idea here to provide the same way of preparing the dataset regardless of whether it’s train set, validation set, or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57427d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_X(df):\n",
    "    df_num = df[base]\n",
    "    df_num = df_num.fillna(0)\n",
    "    X = df_num.values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d3cdf-79ed-4e52-bed7-269288ce9243",
   "metadata": {},
   "source": [
    "Now we can use this function when we prepare data for the training and for the validation as well. In the training part we only use training dataset to train the model. In the validation part we prepare the validation dataset the same way like before and apply the model. Lastly we compute the rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training part:\n",
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression(X_train, y_train)\n",
    "\n",
    "# Validation part:\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w0 + X_val.dot(w)\n",
    "\n",
    "# Evaluation part:\n",
    "rmse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf3b23-f856-4a4b-bde4-c2be73ba48ea",
   "metadata": {},
   "source": [
    "When we compare the RMSE from training with the value from validation (0.73 vs. 0.75) we see that the model performs similarly well on the seen and unseen data. That is what we have hoped for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c98fc",
   "metadata": {},
   "source": [
    "## 2.11 Simple feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4516985-7d38-441d-8748-ac8301de7a39",
   "metadata": {},
   "source": [
    "https://knowmledge.com/2023/09/22/ml-zoomcamp-2023-machine-learning-for-regression-part-9/\n",
    "\n",
    "Suppose we want to develop a new feature based on the existing ones in the feature matrix X. Let’s assume we want to use the year information as an age information. Let’s assume further we have year 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40efec-c9a7-4f99-a0ee-ec98bb353b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "2017 - df_train.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897dae4-a951-472b-a616-e0890252a223",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can add this new feature ‘age’ to our prepare_X function. What is one important remark here. \n",
    "It’s a good way to copy the dataframe inside prepare_X. Otherwise while using df you’ll modify the original data, what ist mostly not wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fe350b-f7d0-43fa-b771-dd9720dd140b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']\n",
    " \n",
    "def prepare_X(df):\n",
    "    df = df.copy()\n",
    "     \n",
    "    df['age'] = 2017 - df.year\n",
    "    features = base + ['age']\n",
    "     \n",
    "    df_num = df[features]\n",
    "    df_num = df_num.fillna(0)\n",
    "    # extracting the Numpy array\n",
    "    X = df_num.values\n",
    "    return X\n",
    " \n",
    "X_train = prepare_X(df_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1add0db-98d2-46bc-b0e1-3851efac0174",
   "metadata": {
    "tags": []
   },
   "source": [
    "The output of the last snippet shows a list of lists. Each list has 6 items, 5 numerical columns and our new ‘age’ column. Let’s train a new model and see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e48bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression(X_train, y_train)\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w0 + X_val.dot(w)\n",
    "rmse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4116da1-551b-478e-ae3f-429ffc0b476e",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see an improvement. The rmse decreased from 0.748 to 0.516. The improvement in the rmse was clear. Let’s see if this improvement can be seen in the plots as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fe728",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_pred, label='prediction', color='red', alpha=0.5, bins=50)\n",
    "sns.histplot(y_val, label='target', color='blue',  alpha=0.5, bins=50)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa8eec-bdd6-4a2d-bf22-4da2fd0a5b99",
   "metadata": {},
   "source": [
    "Here, too, a clear improvement can be seen. Many car prices are predicted much better. But there is still space for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843bce5",
   "metadata": {},
   "source": [
    "## 2.12 Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01cdfe-1f1a-43ad-8ecb-39175b312600",
   "metadata": {},
   "source": [
    "Source: https://knowmledge.com/2023/09/23/ml-zoomcamp-2023-machine-learning-for-regression-part-10/\n",
    "\n",
    "Categorical variables are variables that are categories (typically strings) Here: make, model, engine_fuel_type, transmission_type, driven_wheels, market_category, vehicle_size, vehicle_style "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbbf9d-a69e-4e18-ace4-bc5619145bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3ea01-296f-407c-b8e4-eb4d99830f54",
   "metadata": {},
   "source": [
    "But, there is one value that looks like numerical variable, but it isn’t. number_of_doors is not really a numerical number  (it's float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4eb875-3e0a-4084-a227-c4adf9eeea3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.number_of_doors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57397c-7fb5-41cf-acb6-b4e4429a4423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.number_of_doors == 2 # Find cars which have 2 doors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9033e8-50d9-4ea9-b8f0-1fca9ed41371",
   "metadata": {},
   "source": [
    "Typical way of encoding such categorical variables is that we represent it with a bunch of binary columns – so called one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77e646-55d1-4b1a-a131-0c8cf508dede",
   "metadata": {},
   "source": [
    "We can imitate this encoding by turning the booleans from the last snippet into integers (1 and 0) and creating a new variable for each number of doors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdd364-47ae-45dd-b893-e4d76a8d65ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['num_doors_2'] = (df_train.number_of_doors == 2).astype('int')\n",
    "df_train['num_doors_3'] = (df_train.number_of_doors == 3).astype('int')\n",
    "df_train['num_doors_4'] = (df_train.number_of_doors == 4).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893369d-d1e7-4455-97a2-0df588c0886a",
   "metadata": {},
   "source": [
    "But we can do this easier with string replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3efd25-38fd-49df-b573-ae51ecccb25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'num_doors_%s' % 4\n",
    "# Output: 'num_doors_4'\n",
    " \n",
    "# With that replacement we can write a loop\n",
    "for v in [2, 3, 4]:\n",
    "    df_train['num_doors_%s' % v] = (df_train.number_of_doors == v).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57512b6-2b23-424c-a6d6-df3f7c1c6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We delete this because we'll use another solution\n",
    "for v in [2, 3, 4]:\n",
    "    del df_train['num_doors_%s' % v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4c121-d5cc-4cff-adfd-b64d9f7fcabe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let’s use this string replacement method in our prepare_X function.\n",
    "def prepare_X(df):\n",
    "    df = df.copy()\n",
    "    features = base.copy()\n",
    "     \n",
    "    df['age'] = 2017 - df.year\n",
    "    features.append('age')\n",
    "     \n",
    "    for v in [2, 3, 4]:\n",
    "        df['num_doors_%s' % v] = (df.number_of_doors == v).astype('int')\n",
    "        features.append('num_doors_%s' % v)\n",
    "     \n",
    "    df_num = df[features]\n",
    "    df_num = df_num.fillna(0)\n",
    "    # extracting the Numpy array\n",
    "    X = df_num.values\n",
    "    return X\n",
    " \n",
    "prepare_X(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can check if the model performance has improved with the new features.\n",
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression(X_train, y_train)\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w0 + X_val.dot(w)\n",
    "rmse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09a38b-46c4-4cad-acfc-c76e6e19965d",
   "metadata": {},
   "source": [
    "We see in contrast to the last training with rmse of 0.514 there is only a slightly improvement (0.516 vs 0.514, lower is better), almost negligible. So the number of doors feature is not that useful. Maybe the ‘Make’ information is more useful. (our target is lowering the rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea519bc3-a651-4676-af22-bd3fa0abedc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.make.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45ba5e-cdbc-4008-917f-17bfdbaaba34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1a3c8-6816-4829-94f9-f0be52fcc2a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "There are 48 unique values in the ‘Make’ column. That could be too much. Let’s look at the most popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78936f61-fcd0-4e47-ae0d-edfcb99c155c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.make.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b50853-8048-4981-9fff-7b5230a9ec2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If we want to get the actual values, we use the index property\n",
    "df.make.value_counts().head().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e1ea4-df2d-425d-aad1-8eddddfc6fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap it in a usual Python list\n",
    "makes = list(df.make.value_counts().head().index)\n",
    "makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1c1d7-4d51-44cd-abbc-92dee2195db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can now adapt again our prepare_X function to add the new feature.\n",
    "def prepare_X(df):\n",
    "    df = df.copy()\n",
    "    features = base.copy()\n",
    "     \n",
    "    df['age'] = 2017 - df.year\n",
    "    features.append('age')\n",
    "     \n",
    "    for v in [2, 3, 4]:\n",
    "        df['num_doors_%s' % v] = (df.number_of_doors == v).astype('int')\n",
    "        features.append('num_doors_%s' % v)\n",
    "         \n",
    "    for v in makes:\n",
    "        df['make_%s' % v] = (df.make == v).astype('int')\n",
    "        features.append('make_%s' % v)\n",
    "     \n",
    "    df_num = df[features]\n",
    "    df_num = df_num.fillna(0)\n",
    "    # extracting the Numpy array\n",
    "    X = df_num.values\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10679e47-26c2-4c02-8968-aa7df865ad74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression(X_train, y_train)\n",
    " \n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w0 + X_val.dot(w)\n",
    " \n",
    "rmse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc47e86-5cdf-4846-be8c-9aaf4600a110",
   "metadata": {},
   "source": [
    "The model performance has once again improved somewhat (lowered from 0.514 to 0.507) How about adding all the other categorical variables now? This should improve the performance even more, right? Let’s try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86888ecf-508b-47a1-8a68-4e2061a95bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_variables = [\n",
    "    'make', 'engine_fuel_type', 'transmission_type', 'driven_wheels', \n",
    "    'market_category', 'vehicle_size', 'vehicle_style'\n",
    "]\n",
    " \n",
    "# The dictionary category will contain for each of the categories \n",
    "# the top 5 most common ones\n",
    "categories = {}\n",
    " \n",
    "for c in categorical_variables:\n",
    "    categories[c] = list(df[c].value_counts().head().index)\n",
    "     \n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7127d87-2ea6-4109-850b-1976386f2010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The next snippet shows how to implement the new features to our prepare_X function. This time we need two loops as described inline.\n",
    "\n",
    "def prepare_X(df):\n",
    "    # this is good way to do, otherwise while using df you'll modify the original data\n",
    "    # what is mostly not wanted\n",
    "    df = df.copy()\n",
    "    features = base.copy()\n",
    "     \n",
    "    df['age'] = 2017 - df.year\n",
    "    features.append('age')\n",
    "     \n",
    "    for v in [2, 3, 4]:\n",
    "        df['num_doors_%s' % v] = (df.number_of_doors == v).astype('int')\n",
    "        features.append('num_doors_%s' % v)\n",
    " \n",
    "    # First loop is for each key of the dictionary categories.\n",
    "    # Second loop is for each value inside the categories\n",
    "    # For each of this values we create a new column.\n",
    "    for c, values in categories.items():    \n",
    "        for v in values:\n",
    "            df['%s_%s' % (c, v)] = (df[c] == v).astype('int')\n",
    "            features.append('%s_%s' % (c, v))\n",
    "     \n",
    "    df_num = df[features]\n",
    "    df_num = df_num.fillna(0)\n",
    "    # extracting the Numpy array\n",
    "    X = df_num.values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4a4d0-239e-42b6-94cf-dfcca9d4a0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression(X_train, y_train)\n",
    " \n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w0 + X_val.dot(w)\n",
    " \n",
    "rmse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f997c03-f22e-4ffa-bc06-f8087a787cce",
   "metadata": {
    "tags": []
   },
   "source": [
    "This time the model performance is very bad. As you can see the RMSE (3.1033420378110214e) is very large. So something went wrong. We will see in next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5d41c",
   "metadata": {},
   "source": [
    "## 2.13 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a94da1-8b68-4b82-a6ba-c773fd2cebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://knowmledge.com/2023/09/23/ml-zoomcamp-2023-machine-learning-for-regression-part-11/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b019d-a144-4798-b0cf-fc70bf7746e2",
   "metadata": {},
   "source": [
    "The topic for this part is regularization as a way to solve the problem of duplicated columns in our data. Remember the formula for normal equation is: w = (X^TX)^-1*X^Ty\n",
    "The problem what we have is connected with the first part (X^TX)^-1. We need to take an inverse of the GRAM matrix. Sometimes this inverse doesn’t exist. This happens when there are duplicate features in X. Let's see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You see here 2nd and 3rd columns are identical\n",
    "X = [\n",
    "    [4, 4, 4],\n",
    "    [3, 5, 5],\n",
    "    [5, 1, 1],\n",
    "    [5, 4, 4],\n",
    "    [7, 5, 5],\n",
    "    [4, 5, 5]\n",
    "]\n",
    "\n",
    "X = np.array(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "XTX = X.T.dot(X)\n",
    "XTX"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5dc9e85-b01f-4705-9a24-4613a95c9cf9",
   "metadata": {},
   "source": [
    "The output of the last snippet shows the XTX matrix. You see the 2nd and 3rd columns are the same. In this case the inverse doesn’t exist.\n",
    "\n",
    "Remember: In linear algebra we say that one column is a linear combination of other columns. That means it’s possible to express the column number 3 with other columns of the matrix which is basically just a duplicate of column 2.\n",
    "Therefore the next code snipped raises an error “Singular matrix”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c14a7-61ff-45c6-8f08-940ab4416821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.linalg.inv(XTX) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a125a4-6bc5-44e6-9ad3-6599c51cf171",
   "metadata": {},
   "source": [
    "The code from the last article didn’t raise that error, so the inverse of that gram matrix exists. But the reason for the very big value for rmse is that our data is not very clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3ef84-8441-47ee-8fa8-67524f1118c7",
   "metadata": {},
   "source": [
    "Let’s go back to our last example but this time similar X as before with a few noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc2cf3-0c23-407c-a7a8-b82a12ef97c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = [\n",
    "    [4, 4, 4],\n",
    "    [3, 5, 5],\n",
    "    [5, 1, 1],\n",
    "    [5, 4, 4],\n",
    "    [7, 5, 5],\n",
    "    [4, 5, 5.0000001],   #added little noise, 5.0000001 instead of 5\n",
    "]\n",
    " \n",
    "X = np.array(X)\n",
    "y = [1, 2, 3, 1, 2, 3]\n",
    " \n",
    "XTX = X.T.dot(X)\n",
    "XTX\n",
    " \n",
    "XTX_inv = np.linalg.inv(XTX)\n",
    "XTX_inv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bced9f5c-d930-466e-ad76-2fae6644e7bb",
   "metadata": {},
   "source": [
    "As we can see from this example, a little noise is enough to make the two columns no longer identical. This leads to the fact that the calculation of the gram matrix no longer throws an error. Now we can calculate vector w again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbdb97-05db-4ceb-97b0-50a8bac2ab16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = XTX_inv.dot(X.T).dot(y)\n",
    "w"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59b81b53-c8d0-4095-8348-46c680960e76",
   "metadata": {},
   "source": [
    "The first element (that’s the unique feature) of w looks ok, but the second and the third elements (that are the duplicates with noise) are very big numbers. That’s why we have duplicates in our feature matrix. The noise leads to the fact that no more error is thrown. Nevertheless, the model performs poorly due to the duplicates. What we can do to fix the problem is to add a small number (called alpha) to the diagonal of XTX. Let’s demonstrate this on an easier example of XTX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69bc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a small number to the diagonal\n",
    "# helps to control. So the numbers of w become smaller\n",
    "XTX = [\n",
    "    [1.0001, 2, 2],\n",
    "    [2,     1.0001, 1.0000001],\n",
    "    [2, 1.0000001, 1.0001]\n",
    "]\n",
    "\n",
    "XTX = np.array(XTX)\n",
    "np.linalg.inv(XTX)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15045398-4ebc-4afc-8eee-fade220fb9b2",
   "metadata": {},
   "source": [
    "The larger the number alpha adding to the diagonal, the more we have these weights under control. The reason why this works this way is, that this decrease the likelihood that these two columns are just copies of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae1591-7f18-4dc4-8874-a75eaea4aed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XTX = [\n",
    "    [1.01, 2, 2],\n",
    "    [2,   1.01, 1.0000001],\n",
    "    [2, 1.0000001,   1.01]\n",
    "]\n",
    " \n",
    "XTX = np.array(XTX)\n",
    "np.linalg.inv(XTX)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6248764a-3d6e-49e4-b8f3-a035cbcf1ca8",
   "metadata": {},
   "source": [
    "Let’s implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b0c23-a8cc-4203-a93d-1958da7312b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "XTX = [\n",
    "    [1, 2, 2],\n",
    "    [2, 1, 1.0000001],\n",
    "    [2, 1.0000001, 1]\n",
    "]\n",
    " \n",
    "XTX =  np.array(XTX)\n",
    "XTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae038432-692a-4131-87d5-7dcf39b5745b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remember there was the eye function to get an Identity matrix. Maybe we can use this…\n",
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69846c18-b5fa-41ee-af0e-e6b8caa19723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# When adding XTX to this matrix, it adds one on the diagonal\n",
    "XTX + np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffac923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can multiply this eye by a small number\n",
    "XTX = XTX + 0.01 * np.eye(3)\n",
    "XTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbefeba-e56c-44dd-9adc-30d6b0b90e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XTX = XTX + 0.1*np.eye(3)\n",
    "# XTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65d505-288e-47c1-ac87-47852a1a10fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XTX = XTX + 1*np.eye(3)\n",
    "# XTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca916c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(XTX)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcd48ec0-b6aa-40dd-8096-64ad425b5794",
   "metadata": {},
   "source": [
    "Solving this problem is called regularization and means in this case controlling. We’re controlling the weights that they don’t grow too much. Alpha = 0.01 is a parameter, and the larger this parameter the larger the numbers on the diagonal. And the larger this numbers on the diagonal the smaller the values in the inverse XTX matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This leads us to reimplementing the train_linear_regression function again\n",
    "# reg = regularized\n",
    "# parameter r = short for regularization\n",
    "def train_linear_regression_reg(X, y, r=0.001):\n",
    "    ones = np.ones(X.shape[0])\n",
    "    X = np.column_stack([ones, X])\n",
    "\n",
    "    XTX = X.T.dot(X)\n",
    "    XTX = XTX + r * np.eye(XTX.shape[0])\n",
    "\n",
    "    XTX_inv = np.linalg.inv(XTX)\n",
    "    w_full = XTX_inv.dot(X.T).dot(y)\n",
    "    \n",
    "    return w_full[0], w_full[1:]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8098131-7a1e-470d-a138-1ff35b6e9032",
   "metadata": {},
   "source": [
    "To test the effect of regularization, we need to re-train our model and apply it to the validation data. Then we can calculate the rmse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression_reg(X_train, y_train, r=0.01)\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w0 + X_val.dot(w)\n",
    "rmse(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2a1448c-6a06-48e0-8ffa-f363e6bf54cb",
   "metadata": {},
   "source": [
    "This is the best result we had before, but we don’t know that there is no better one.\n",
    "To find a good value for r, let's see next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfc7cf",
   "metadata": {},
   "source": [
    "## 2.14 Tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce25b7-fb04-4f55-8845-448c4a88bd75",
   "metadata": {},
   "source": [
    "Source: https://knowmledge.com/2023/09/24/ml-zoomcamp-2023-machine-learning-for-regression-part-12/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "227ced26-848e-4107-b954-3d9b4ccce86e",
   "metadata": {},
   "source": [
    "Now we will try to find the best regularization parameter for our linear regression model. We realized that the parameter r affects the quality of our model and now we try to find the best value for this r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f064968",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in [0.0, 0.00001, 0.0001, 0.001, 0.1, 1, 10]:\n",
    "    X_train = prepare_X(df_train)\n",
    "    w0, w = train_linear_regression_reg(X_train, y_train, r=r)\n",
    "\n",
    "    X_val = prepare_X(df_val)\n",
    "    y_pred = w0 + X_val.dot(w)\n",
    "    score = rmse(y_val, y_pred)\n",
    "    \n",
    "    print(\"reg parameter: \",r, \"bias term: \",w0, \"rmse: \",score)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "680fa198-5bf7-4b9a-bb53-f9f171440b6b",
   "metadata": {},
   "source": [
    "What you see here is using r=0 makes the bias term huge and the rmse score aswell.\n",
    "0.001 could be a good parameter for r (since bias term and rmse is lower comparatively others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ad191",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.001\n",
    "X_train = prepare_X(df_train)\n",
    "w0, w = train_linear_regression_reg(X_train, y_train, r=r)\n",
    "\n",
    "X_val = prepare_X(df_val)\n",
    "y_pred = w0 + X_val.dot(w)\n",
    "score = rmse(y_val, y_pred)\n",
    "print(\"rmse: \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e538f",
   "metadata": {},
   "source": [
    "## 2.15 Using the model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa654c24-20bc-4a0f-80e8-f9ffde3b4306",
   "metadata": {},
   "source": [
    "Previously, we found the best parameter for the linear regression and in this lesson we’ll train the model again and use it. What we did so far is, we trained our model on training dataset and applied the best model on validation dataset. To check the model performance we calculated the RMSE.\n",
    "\n",
    "What we want to do now is to train our final model on both training dataset and validation dataset. We call this FULL TRAIN. After that we make the final evaluation on the test dataset to make sure that our model works fine and check what is the value for RMSE. It shouldn’t be too different from what we saw on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade139b-3654-404f-a127-bc523f77fc62",
   "metadata": {},
   "source": [
    "### Combining datasets\n",
    "First step to do is getting our data. So we need to combine df_train and df_val into one dataset. We can use Pandas concat() function that takes a list of dataframes and concatenates them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train = pd.concat([df_train, df_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d65d5e-b27c-4bd7-8008-c3887707a550",
   "metadata": {},
   "source": [
    "We also need to concatenate y_train and y_val to get y_full_train. This time we use the concatenate function of NumPy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016e8ef-a5b3-485e-9ead-d9603737406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full_train = np.concatenate([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86990f88-9b2a-4d61-9ebf-3e698035b422",
   "metadata": {},
   "source": [
    "### Resetting index\n",
    "When combining two dataframes it can happen that the index is not sequential. Here you can use an already known function and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train = df_full_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629f668-3af6-4506-9eb6-b7a3344750b0",
   "metadata": {},
   "source": [
    "### Getting feature matrix X\n",
    "Now we have again a coherent dataset for training and we can prepare it for the usage as we did before. The prepare_X() function still works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59473aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_train = prepare_X(df_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a24a4-e19f-4140-9a74-d11d6bee3f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_full_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b1a8b-cbd1-4552-8d4f-0942f15be1f6",
   "metadata": {},
   "source": [
    "### Train the final model\n",
    "Next step is to train the final model on the combined dataset. We’re using the new train_linear_regression_reg() function to get the value for w0 and the vector w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd0a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w = train_linear_regression_reg(X_full_train, y_full_train, r=0.001)\n",
    "w0, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b05057-b9ae-4242-ba05-077d9ffa0a5d",
   "metadata": {},
   "source": [
    "### Applying model to test data\n",
    "Now is the great moment for the final model. It must pass the final test. For this purpose we use test data, which are again prepared with the prepare_X() function. Then the model is applied to the test data and the RMSE can be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff768c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = prepare_X(df_test)\n",
    "y_pred = w0 + X_test.dot(w)\n",
    "\n",
    "score = rmse(y_test, y_pred)\n",
    "\n",
    "print(\"rmse: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bfeb3f-2cae-4385-9166-2d0376a61237",
   "metadata": {},
   "source": [
    "RMSE_test = 0.4629526324225134 is not so far away from RMSE_val = 0.4549809533999964. That means the model generalizes quite well and it didn’t get this score by chance. Now we have our final model and we can use it. The way we want to use it is to predict the price of an (unseen) car – unseen means here that the model hasn’t seen this car during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4053f1-f36d-416c-8873-b415e9a76f52",
   "metadata": {},
   "source": [
    "### Using the model\n",
    "Using the model means:\n",
    "\n",
    "- Extracting all the features (getting feature vector of the car)\n",
    "- Applying our final model to this feature vector & predicting the price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a4b92-39d1-4cf6-a8aa-99b9fc8f0489",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "For this step we can take any car from our test dataset and pretend it’s a new car. Let’s just take one car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200592d-1ec8-47fa-a8ed-14da46019ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.iloc[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74c857-7edf-41d7-a763-89c129c98163",
   "metadata": {},
   "source": [
    "Usually the way we do it is that we don’t get a dataframe here. But it could be a Python dictionary with all the information about the car. In real life you can imagine a website or an app, where people enter all the values. Then the website sends the request with all the information (as dictionary) to the model. The model replies back with the predicted price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ef87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example we turn this data of our car into a dictionary.\n",
    "car = df_test.iloc[20].to_dict()\n",
    "car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The car is our request and now remember the prepare_X function expects a dataframe, so we need to create a dataframe with a single row for our request.\n",
    "df_small = pd.DataFrame([car])\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this single row DataFrame as input for the prepare_X() function to get the feature matrix. In this case our feature matrix is a feature vector.\n",
    "X_small = prepare_X(df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e311f-1edc-4297-a7ac-39625f2341df",
   "metadata": {},
   "source": [
    "#### Predicting the price\n",
    "The final step is to apply the final model to our requested car (feature vector) and predict the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f16380",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w0 + X_small.dot(w)\n",
    "# Don't need an array but it's first (and only) item\n",
    "y_pred = y_pred[0]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9858f60-26e7-4b0a-ba39-6e029d8b2d5e",
   "metadata": {},
   "source": [
    "10.67 is still not the price in $. To get the real price we need to undo the logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054baebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expm1(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70fd94-5b0e-4bae-9c1e-2042f4b911f4",
   "metadata": {},
   "source": [
    "After undoing the logarithm we get the price in $. \n",
    "\n",
    "So we think that a car with these characteristics should cost  $43,432.70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly to get an evaluation about model performance let’s compare the predicted price to the actual price of this requested car\n",
    "np.expm1(y_test[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24063f63-a304-4619-b632-0993ebdd468e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Original price is $50,549.99; not bad prediction comparatively $30,973.55 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39663b05",
   "metadata": {},
   "source": [
    "## 2.16 Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbdf501",
   "metadata": {},
   "source": [
    "* We included only 5 top features. What happens if we include 10?\n",
    "\n",
    "Other projects\n",
    "\n",
    "* Predict the price of a house - e.g. boston dataset\n",
    "* https://archive.ics.uci.edu/ml/datasets.php?task=reg\n",
    "* https://archive.ics.uci.edu/ml/datasets/Student+Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10287cec",
   "metadata": {},
   "source": [
    "## 2.17 Summary\n",
    "\n",
    "* EDA - looking at data, finding missing values\n",
    "* Target variable distribution - long tail => bell shaped curve\n",
    "* Validation framework: train/val/test split (helped us detect problems)\n",
    "* Normal equation - not magic, but math\n",
    "* Implemented it with numpy\n",
    "* RMSE to validate our model\n",
    "* Feature engineering: age, categorical features\n",
    "* Regularization to fight numerical instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb7a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd20305-f6fd-482b-b044-3263213ecd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlzoomcamp",
   "language": "python",
   "name": "mlzoomcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
