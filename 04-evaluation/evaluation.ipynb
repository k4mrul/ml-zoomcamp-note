{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb05700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b9b4a",
   "metadata": {},
   "source": [
    "# 4. Evaluation Metrics for Classification\n",
    "\n",
    "In the previous session we trained a model for predicting churn. How do we know if it's good?\n",
    "\n",
    "\n",
    "## 4.1 Evaluation metrics: session overview \n",
    "\n",
    "* Dataset: https://www.kaggle.com/blastchar/telco-customer-churn\n",
    "* https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
    "\n",
    "\n",
    "*Metric* - function that compares the predictions with the actual values and outputs a single number that tells how good the predictions are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff7c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66e768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ee3ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the dataset and store it in the 'df' dataframe\n",
    "df = pd.read_csv('data-week-3.csv')\n",
    "\n",
    "# Standardize column names by converting them to lowercase and replacing spaces with underscores\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Identify categorical columns (columns with object data type)\n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    "\n",
    "# Standardize categorical column values by converting them to lowercase and replacing spaces with underscores\n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Convert 'totalcharges' column to numerical format (it was mistakenly identified as categorical)\n",
    "# 'errors=\"coerce\"' ensures that non-numeric values are turned into NaN instead of raising errors\n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "\n",
    "# Replace NaN values in 'totalcharges' with 0\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    "\n",
    "# Convert 'churn' column to integer format: 'yes' -> 1, 'no' -> 0\n",
    "df.churn = (df.churn == 'yes').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1903b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into 'df_full_train' (80%) and 'df_test' (20%)\n",
    "# 'random_state=1' ensures the split is reproducible\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "# Further split 'df_full_train' into 'df_train' (60%) and 'df_val' (20%)\n",
    "# This results in an overall split of 60% training, 20% validation, and 20% testing\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "\n",
    "# Reset the indices for all datasets to ensure they have continuous indices\n",
    "# 'drop=True' removes the old index to avoid confusion\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "# Extract the target variable ('churn') from each dataset\n",
    "y_train = df_train.churn.values\n",
    "y_val = df_val.churn.values\n",
    "y_test = df_test.churn.values\n",
    "\n",
    "# Remove the 'churn' column from feature datasets to prevent data leakage during training\n",
    "del df_train['churn']\n",
    "del df_val['churn']\n",
    "del df_test['churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4132a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‘numerical’ and ‘categorical’ contain the relevant column names. The ‘numerical’ array contains the names of all numerical columns, \n",
    "# while the ‘categorical’ array contains the names of all categorical columns.\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "categorical = [\n",
    "    'gender',\n",
    "    'seniorcitizen',\n",
    "    'partner',\n",
    "    'dependents',\n",
    "    'phoneservice',\n",
    "    'multiplelines',\n",
    "    'internetservice',\n",
    "    'onlinesecurity',\n",
    "    'onlinebackup',\n",
    "    'deviceprotection',\n",
    "    'techsupport',\n",
    "    'streamingtv',\n",
    "    'streamingmovies',\n",
    "    'contract',\n",
    "    'paperlessbilling',\n",
    "    'paymentmethod',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4583b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of DictVectorizer with sparse=False to return dense arrays\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Convert the training dataframe into a list of dictionaries (one dictionary per record)\n",
    "# This format is required by DictVectorizer to process the data\n",
    "train_dict = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "# Fit the DictVectorizer to the training data and transform it into a feature matrix\n",
    "# 'fit_transform' learns the structure of the data, including column names and values\n",
    "# It performs one-hot encoding for categorical values but ignores numeric values\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model using the transformed training data and corresponding target values\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea51f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the validation DataFrame into a list of dictionaries (one dictionary per record)\n",
    "# This transformation is necessary for DictVectorizer to process the validation data\n",
    "val_dict = df_val[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "# Transform the validation data using the trained DictVectorizer\n",
    "# Unlike training, we only use 'transform' here since the DictVectorizer is already fitted\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "# Predict probabilities using the trained model\n",
    "# 'predict_proba' returns two columns: probability of class 0 and class 1\n",
    "# We extract the second column, which represents the probability of churn (class 1)\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Make churn decisions based on a threshold of 0.5\n",
    "# If the predicted probability is >= 0.5, churn_decision is True; otherwise, it is False\n",
    "churn_decision = (y_pred >= 0.5)\n",
    "\n",
    "# Calculate the model's accuracy by comparing predicted churn decisions with actual values\n",
    "# The 'mean' function computes the proportion of correct predictions\n",
    "(y_val == churn_decision).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a7559",
   "metadata": {},
   "source": [
    "## 4.2 Accuracy and dummy model\n",
    "\n",
    "source: https://knowmledge.com/2023/10/03/ml-zoomcamp-2023-evaluation-metrics-for-classification-part-2/\n",
    "\n",
    "#### In the last article, we calculated that our model achieved an accuracy of 80% on the validation data. \n",
    "#### Now, let’s determine whether this is a good value or not.\n",
    "\n",
    "### What we will do\n",
    "\n",
    "* Evaluate the model on different thresholds\n",
    "* Check the accuracy of dummy baselines"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bba69dd-3ff1-4d7c-9eeb-808621cb9a7c",
   "metadata": {},
   "source": [
    "We evaluated the model’s predictions on the validation dataset using a 0.5 threshold for churn. Customers with predicted values ≥ 0.5 were classified as churning, while those below were non-churning. Out of 1409 customers, 1132 were correctly predicted, resulting in an accuracy of 80% (1132/1409). Whether this accuracy is sufficient depends on the context and problem requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948eda88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59dff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(y_val == churn_decision).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcb4def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1132/ 1409"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe349a05-da54-4d43-af6b-40a597809ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate the model on different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1012b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb75fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_val, y_pred >= 0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19b46c4c-3bd9-4463-af4f-781c2251a75c",
   "metadata": {},
   "source": [
    "To determine the best threshold, we can adjust it and revalidate the model. Using NumPy’s linspace, we generate multiple threshold values (e.g., 21 evenly spaced between 0 and 1). For each threshold, we calculate accuracy and select the best one based on validation results. This fine-tuning helps optimize the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27840301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 21)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    score = accuracy_score(y_val, y_pred >= t)\n",
    "    print('%.2f %.3f' % (t, score))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90e1c343-f32a-4a4c-8a4a-bdd22d980840",
   "metadata": {},
   "source": [
    "The validation results confirm that 0.5 is the best threshold, making it a suitable choice for our model. To visualize the optimization process, we can plot threshold values on the x-axis and accuracy (or another metric) on the y-axis. This helps illustrate how model performance changes with different thresholds, making it easier to identify the optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f492a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0a4bf-ea03-4d2c-b3e1-1ba07a74b72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check Accuracy of Dummy Baseline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f2562b0-db9b-48ff-8e5d-39e9965a1624",
   "metadata": {},
   "source": [
    "There is an important point about the limitations of accuracy as an evaluation metric. Accuracy alone may not fully reflect a model’s performance, especially with imbalanced datasets or when certain errors are more critical. While our model has 80% accuracy, a dummy model predicting all customers as non-churning still achieves 73%, showing accuracy’s limitations. In churn prediction, false negatives (missed churners) are often more costly than false positives. The best evaluation metric depends on the problem—when false negatives matter more, metrics like recall may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b048b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0897055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Counter(y_pred >= 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d04fb-475a-423b-b37c-54055c18083b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Distribution of y_val\n",
    "Counter(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbe134-d655-46c4-93e3-cead11eb3aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1023/1409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79c972-20db-437b-b825-23656cdd1722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ece7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1 - y_val.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79919865-30db-4c7f-a96f-df092a3d7f9b",
   "metadata": {},
   "source": [
    "The dataset has a class imbalance, with 27% churning customers and 73% non-churning customers. This makes accuracy misleading, as a dummy model predicting only the majority class could still achieve high accuracy while failing to identify the minority class (churning customers). To address this, alternative metrics should be used:\n",
    "\n",
    "* Precision: Measures true positive predictions among all positive predictions, useful when false positives are costly.\n",
    "* Recall: Measures true positive predictions among actual positives, important when false negatives are costly.\n",
    "* F1-Score: Balances precision and recall, considering both false positives and negatives.\n",
    "* AUC-ROC: Assesses the ability to distinguish between classes, particularly helpful for imbalanced datasets.\n",
    "\n",
    "\n",
    "Choosing the right metric depends on the problem’s goals, with an emphasis on accurately identifying the minority class in cases of imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1600912",
   "metadata": {},
   "source": [
    "## 4.3 Confusion table / matrix\n",
    "\n",
    "(source: https://knowmledge.com/2023/10/04/ml-zoomcamp-2023-evaluation-metrics-for-classification-part-3/)\n",
    "\n",
    "A confusion matrix is a tool used to check how well a classification model is working. It helps us see the mistakes and correct predictions the model makes.\n",
    "\n",
    "Sometimes, using only accuracy can be misleading, especially if the classes are imbalanced (one class appears much more than the other). The confusion matrix gives a better way to evaluate the model by breaking predictions into four types:\n",
    "* True Positives (TP): The model correctly predicted the positive class (churning customers).\n",
    "* True Negatives (TN): The model correctly predicted the negative class (non-churning customers).\n",
    "* False Positives (FP): The model wrongly predicted the positive class (Type I error).\n",
    "* False Negatives (FN): The model wrongly predicted the negative class (Type II error).\n",
    "\n",
    "\n",
    "| g(xi) < t (NEGATIVE – NO CHURN) | g(xi) < t (NEGATIVE – NO CHURN) | g(xi) >= t (POSITIVE – CHURN) | g(xi) >= t (POSITIVE – CHURN) |\n",
    "|----------------------------------|----------------------------------|--------------------------------|--------------------------------|\n",
    "| **C didn’t churn** | **C churned** | **C didn’t churn** | **C churned** |\n",
    "| correct | incorrect | incorrect | correct |\n",
    "| **TRUE NEGATIVE (TN)** | **FALSE NEGATIVE (FN)** | **FALSE POSITIVE (FP)** | **TRUE POSITIVE (TP)** |\n",
    "| g(xi) < t & y = 0 | g(xi) < t & y = 1 | g(xi) >= t & y = 0 | g(xi) >= t & y = 1 |\n",
    "\n",
    "\n",
    "\n",
    "Here, we will do the following:\n",
    "\n",
    "* Different types of errors and correct decisions\n",
    "* Arranging them in a table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59f747-f81b-4786-aaa4-6a8882c68a6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Different types of errors and correct decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501711e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# people who are going to churn\n",
    "actual_positive = (y_val == 1)\n",
    "# people who are not going to churn\n",
    "actual_negative = (y_val == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898b9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = 0.5\n",
    "predict_positive = (y_pred >= t)\n",
    "predict_negative = (y_pred < t)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f35b41c-7fea-4f6c-8dd4-8f6e24991222",
   "metadata": {},
   "source": [
    "We examine the cases where both “predict_positive” and “actual_positive” are true. This is precisely what the “&” operator represents, indicating a logical AND operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46cfef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_positive & actual_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0d18b9-c2e5-4dd9-beff-f03f117371d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tp = (predict_positive & actual_positive).sum()\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae36df-bd2b-495a-97d4-b55c49e02e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tn = (predict_negative & actual_negative).sum()\n",
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791ab07-440e-4006-88f8-0902880be87b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp = (predict_positive & actual_negative).sum()\n",
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767961e-de5f-4a19-a359-278d5c053ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn = (predict_negative & actual_positive).sum()\n",
    "fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e7ce1-c8e7-4017-9569-24d2c8b63086",
   "metadata": {},
   "source": [
    "### Arranging them in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc068bcf-b764-4558-8a62-f4175c78c8e1",
   "metadata": {},
   "source": [
    "That was preparation for understanding the confusion matrix. The confusion matrix is a way to consolidate all these values (tp, tn, fp, fn) into a single table. This table comprises 4 cells, forming a 2×2 matrix.\n",
    "\n",
    "* In the columns of this table, we have the predictions (NEGATIVE: g(xi) < t and POSITIVE: g(xi) >= t).\n",
    "* In the rows, we have the actual values (NEGATIVE: y=0 and POSITIVE: y=1).\n",
    "\n",
    "Now, let’s proceed to implement this confusion matrix in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f4ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix = np.array([\n",
    "    [tn, fp],\n",
    "    [fn, tp]\n",
    "])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129849f5-f0f3-4932-9bdf-257c4fc8fee8",
   "metadata": {},
   "source": [
    "|                | **NO CHURN** <br> *(g(xi) < t) NEGATIVE* | **CHURN** <br> *(g(xi) >= t) POSITIVE* |\n",
    "|:--------------:|:--------------------------------:|:-----------------------------:|\n",
    "| **NO CHURN** <br> *y=0 NEGATIVE* | **True Negative TN** <br> 922 <br> **65%** | **False Positive FP** <br> 101 <br> **8%** |\n",
    "| **CHURN** <br> *y=1 POSITIVE*   | **False Negative FN** <br> 176 <br> **12%** | **True Positive TP** <br> 210 <br> **15%** |\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3be4925-2168-480c-a5ea-27fc79de0ba2",
   "metadata": {},
   "source": [
    "We observe that we have more false negatives than false positives. False positives represent customers who receive the email even though they are not likely to churn, resulting in a loss of money due to unnecessary discounts. False negatives are customers who do not receive the email and end up leaving, causing financial losses as well. Both situations are undesirable."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d13d96a-5e94-458c-b6fc-1b7efb41f516",
   "metadata": {
    "tags": []
   },
   "source": [
    "Instead of using absolute numbers, we can also express these values in relative terms to gain a better perspective on the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7aab64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(confusion_matrix / confusion_matrix.sum()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b1d4c",
   "metadata": {},
   "source": [
    "## 4.4 Precision and Recall\n",
    "\n",
    "(source: https://knowmledge.com/2023/10/05/ml-zoomcamp-2023-evaluation-metrics-for-classification-part-4/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc355516-8b6c-444f-a270-7bd9d003a81c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Precision and Recall are essential metrics for evaluating binary classification models.\n",
    "\n",
    "**Precision** measures the fraction of positive predictions that were correct. In other words, it quantifies how accurately the model predicts customers who are likely to churn.\n",
    "\n",
    "Precision = True Positives / (# Positive Predictions) = True Positives / (True Positives + False Positives)\n",
    "\n",
    "**Recall**, on the other hand, quantifies the fraction of actual positive cases that were correctly identified by the model. It assesses how effectively the model captures all customers who are actually churning.\n",
    "\n",
    "Recall = True Positives / (# Positive Observations) = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "In summary, precision focuses on the accuracy of positive predictions, while recall emphasizes the model’s ability to capture all positive cases. These metrics are crucial for understanding the trade-offs between correctly identifying churning customers and minimizing false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7efef7-3265-4c2d-a9a7-5816f258ceba",
   "metadata": {},
   "source": [
    "| **Actual Values**        | **Negative Predictions** <br> *(g(xi) < t)* | **Positive Predictions** <br> *(g(xi) >= t)* |\n",
    "|:------------------------:|:--------------------------------:|:-----------------------------:|\n",
    "| **Negative Example** <br> *y=0* | **TN** | **FP** |\n",
    "| **Positive Example** <br> *y=1* | **FN** | **TP** |\n",
    "<p style=\"text-align: center\">Confusion Matrix Recall = TP / (TP + FN) &nbsp;&nbsp; Precision = TP / (TP + FP)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e6897-4812-409a-bf09-5a21951f245f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330cba53-72d0-4125-8631-3c65e141f28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db2636-7a33-495f-84af-4b740e261bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --> promotional email goes to 311 people, but 210 are actually going to churn (--> 33% are mistakes)\n",
    "tp + fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccdb200-38c9-4cf8-9951-96304aed93bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recall = tp / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de309c-c353-4e35-bb47-bcccdf0d63fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --> For 46% of people who are churning we failed to identify them\n",
    "tp + fn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eef81b8b-1e24-447e-b569-04a274dc1ba9",
   "metadata": {},
   "source": [
    "Accuracy alone can be misleading, especially with class imbalance. **Precision** and **recall** give a clearer picture by measuring how well the model identifies positive cases (churning customers).  \n",
    "\n",
    "When it's crucial to detect specific cases—like preventing customer churn—precision and recall help balance correctly identifying positives while minimizing false positives and false negatives. Relying only on accuracy may not fully reflect the model’s effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088b30e",
   "metadata": {},
   "source": [
    "## 4.5 ROC Curves\n",
    "\n",
    "(source: https://knowmledge.com/2023/10/06/ml-zoomcamp-2023-evaluation-metrics-for-classification-part-5/)\n",
    "\n",
    "ROC (Receiver Operating Characteristic) curves help evaluate how well a binary classification model distinguishes between two classes. They show the trade-off between false positives and true positives at different decision thresholds.\n",
    "\n",
    "The curve is created by plotting True Positive Rate (TPR) against False Positive Rate (FPR) for different thresholds. The AUC-ROC (Area Under the Curve - ROC) measures overall performance—higher values mean better model discrimination.\n",
    "\n",
    "ROC curves help choose the best threshold by balancing false positives and true positives based on the problem’s needs.\n",
    "\n",
    "| Actual Values     | Negative Predictions (g(xᵢ) < t) | Positive Predictions (g(xᵢ) ≥ t) |\n",
    "|:----------------:|:--------------------------------:|:--------------------------------:|\n",
    "| **Negative Example (y=0)** | **TN** | **FP** |\n",
    "|  |  | **FPR = FP / (TN + FP)** |\n",
    "| **Positive Example (y=1)** | **FN** | **TP** |\n",
    "|  |  | **TPR = TP / (FN + TP)** |\n",
    "\n",
    "<p style=\"text-align: center\">Confusion matrix: FPR – False Positive Rate, TPR – True Positive Rate</p>\n",
    "\n",
    "\n",
    "### TPR and FRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b9350d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tpr = tp / (tp + fn)\n",
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b418e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr = fp / (fp + tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e1021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The ROC curve is a useful visualization tool that allows you to assess the performance of a binary classification model across a range of decision thresholds.\n",
    "\n",
    "scores = []\n",
    "\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "for t in thresholds:\n",
    "    actual_positive = (y_val == 1)\n",
    "    actual_negative = (y_val == 0)\n",
    "    \n",
    "    predict_positive = (y_pred >= t)\n",
    "    predict_negative = (y_pred < t)\n",
    "\n",
    "    tp = (predict_positive & actual_positive).sum()\n",
    "    tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "    fp = (predict_positive & actual_negative).sum()\n",
    "    fn = (predict_negative & actual_positive).sum()\n",
    "    \n",
    "    scores.append((t, tp, fp, fn, tn))\n",
    "# scores"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0d5705d-2281-4f71-8ba8-cad039997038",
   "metadata": {},
   "source": [
    "We end up with 101 confusion matrices evaluated for different thresholds. Let’s turn that into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76beba9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n",
    "df_scores = pd.DataFrame(scores, columns=columns)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e9aab5-cdd6-4847-a2eb-1d80a41f2f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can look at each tenth record by using this column 10 operator. This works by printing every record starting from the \n",
    "# first record and moving forward with increments of 10.\n",
    "df_scores[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc6a41-c4ad-4f59-a20f-089e86b081f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\n",
    "df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\n",
    "df_scores[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125f2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR')\n",
    "plt.plot(df_scores.threshold, df_scores['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dc644",
   "metadata": {},
   "source": [
    "### Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4b466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "y_rand = np.random.uniform(0, 1, size=len(y_val))\n",
    "y_rand.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c24296",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Accuracy for our random model is around 50%\n",
    "((y_rand >= 0.5) == y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13786f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Let’s put the previously used code into a function.\n",
    "def tpr_fpr_dataframe(y_val, y_pred):\n",
    "    scores = []\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "\n",
    "    for t in thresholds:\n",
    "        actual_positive = (y_val == 1)\n",
    "        actual_negative = (y_val == 0)\n",
    "\n",
    "        predict_positive = (y_pred >= t)\n",
    "        predict_negative = (y_pred < t)\n",
    "\n",
    "        tp = (predict_positive & actual_positive).sum()\n",
    "        tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "        fp = (predict_positive & actual_negative).sum()\n",
    "        fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "        scores.append((t, tp, fp, fn, tn))\n",
    "\n",
    "    columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n",
    "    df_scores = pd.DataFrame(scores, columns=columns)\n",
    "\n",
    "    df_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\n",
    "    df_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\n",
    "    \n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af4bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rand = tpr_fpr_dataframe(y_val, y_rand)\n",
    "df_rand[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea930c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR')\n",
    "plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58a51c60-5a7f-4f1c-9ac9-383db8e0c1a6",
   "metadata": {},
   "source": [
    "Let’s examine an example using a threshold of 0.6. On the x-axis, we have our thresholds, and when we set the threshold to 0.6, we obtain a True Positive Rate (TPR) of 0.4 and a False Positive Rate (FPR) of 0.4.\n",
    "\n",
    "The reason behind these values is that our model’s predictions are nearly equivalent to tossing a coin. In 60% of cases, the model predicts that a customer is non-churning, and in 40% of cases, it predicts that the customer is churning. In other words, these rates indicate that the model predicts a customer as churning with a 40% probability and as non-churning with a 60% probability. Consequently, the model is incorrect for non-churning customers in 40% of cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958bf4d",
   "metadata": {},
   "source": [
    "### Ideal model\n",
    "\n",
    "Now, let’s discuss the concept of an ideal model that makes correct predictions for every example. To implement this, we need to determine the number of negative examples, which corresponds to the number of people who are not churning in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b21df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_neg = (y_val == 0).sum()\n",
    "num_pos = (y_val == 1).sum()\n",
    "num_neg, num_pos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "765d6ee1-44bf-497a-9430-13c76e3ec042",
   "metadata": {},
   "source": [
    "To create the ideal model’s predictions for our validation set, we first create a y_ideal array that contains only negative observations (0s) followed by positive observations (1s). We use the np.repeat() function to achieve this, creating an array with 1023 zeros and then 386 ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f051585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_ideal = np.repeat([0, 1], [num_neg, num_pos])\n",
    "y_ideal"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13ba9e33-6e4e-4a13-bb24-bae5f780a0c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "To create our predictions for the ideal model, which are numbers between 0 and 1, we can use the np.linspace() function to generate an array of evenly spaced values between 0 and 1. This array should have the same length as y_ideal, which is 1409 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a584a57-daff-43a0-9414-efb8b42a9a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_ideal_pred = np.linspace(0, 1, len(y_val))\n",
    "y_ideal_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9c21f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1 - y_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858f4cd-6445-4360-a61e-2dae2f45760d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_ideal = ((y_ideal_pred >= 0.726) == y_ideal).mean()\n",
    "accuracy_ideal"
   ]
  },
  {
   "cell_type": "raw",
   "id": "623a62ff-6e37-40e0-b24d-2f966fd515c0",
   "metadata": {},
   "source": [
    "The ideal model, which makes perfect predictions, doesn’t exist in reality, but it serves as a benchmark to understand how well our actual model is performing. By comparing our model’s performance to that of the ideal model, we can assess how much room for improvement there is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30738fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ideal = tpr_fpr_dataframe(y_ideal, y_ideal_pred)\n",
    "df_ideal[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb9f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR')\n",
    "plt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "313cf040-90dd-4088-a108-1d6b4fb81116",
   "metadata": {},
   "source": [
    "What we see here is that TPR almost always stays around 1 and starts to go down after the threshold of 0.726. So, this model can correctly identify churning customers up to that threshold. For people who are not churning but are classified as churning by the model when the threshold is below 0.726, the model is not always correct. However, the detection becomes always true after the threshold of 0.726.\n",
    "\n",
    "Let’s take another example with a threshold of 0.4. The FPR is around 45%, and the model makes some mistakes. So, for around 32% of people who are predicted as non-churning when the threshold is set to 0.726 but are below that threshold, we predict them as churning even though they are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d98c9",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "Now let’s try to plot all the models together so we can hold the benchmarks together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a28486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(df_scores.threshold, df_scores['tpr'], label='TPR', color='black')\n",
    "plt.plot(df_scores.threshold, df_scores['fpr'], label='FPR', color='blue')\n",
    "\n",
    "# plt.plot(df_rand.threshold, df_rand['tpr'], label='TPR random', color='grey')\n",
    "# plt.plot(df_rand.threshold, df_rand['fpr'], label='FPR random', color='grey')\n",
    "\n",
    "plt.plot(df_ideal.threshold, df_ideal['tpr'], label='TPR ideal')\n",
    "plt.plot(df_ideal.threshold, df_ideal['fpr'], label='FPR ideal')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8678e6d2-6a0d-4db4-afab-f35a7ec9a0d5",
   "metadata": {},
   "source": [
    "We see that our TPR is far from the ideal model. We want it to be as close as possible to 1. We also notice that our FPR is significantly different from that of the ideal model. Plotting against the threshold is not always intuitive. For example, in our model, the best threshold is 0.5, as we know from accuracy. However, for the ideal model, as we saw earlier, the best threshold is 0.726. So they have different thresholds. What we can do to better visualize this is to plot FPR against TPR. On the x-axis, we’ll have FPR, and on the y-axis, we’ll have TPR. To make it easier to understand, we can also add the benchmark lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc605391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(df_scores.fpr, df_scores.tpr, label='Model')\n",
    "plt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0543eae3-d0ee-4159-a858-c91eea413a60",
   "metadata": {},
   "source": [
    "In the curve of the ideal model, there is one crucial point, often referred to as the ‘north star’ or ideal spot, located in the upper-left corner where TPR is 100% and FPR is 0%. This point represents the optimal performance we aim to achieve with our model. A ROC curve visualizes this by plotting TPR against FPR, and we usually add a diagonal random baseline. Our goal is to make our model’s curve as close as possible to this ideal spot, which means simultaneously being as far away as possible from the random baseline. In essence, if our model closely resembles the random baseline model, it is not performing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863648f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### We can also use the ROC functionality of scikit learn package\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.plot(fpr, tpr, label='Model')\n",
    "plt.plot([0, 1], [0, 1], label='Random', linestyle='--')\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38598bdb-5bbf-47f4-ba06-ac097c22182c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### What kind of information do we get from ROC curve?\n",
    "\n",
    "ROC curves show how a model works at different thresholds. Here’s a simple breakdown:\n",
    "\n",
    "1. **Starting Point (Lower-Left Corner):**\n",
    "   - **Threshold = 1.0**: The model is very strict. It predicts *no one* will churn (all are non-churning).\n",
    "   - **TPR (True Positive Rate) = 0**: No churning customers are correctly identified.\n",
    "   - **FPR (False Positive Rate) = 0**: No mistakes (no non-churning customers are wrongly labeled as churning).\n",
    "\n",
    "2. **Moving Right (Lower Thresholds):**\n",
    "   - As the threshold drops, the model predicts *more* customers will churn.\n",
    "   - **TPR increases**: More churning customers are correctly found.\n",
    "   - **FPR increases**: More non-churning customers are wrongly labeled as churning.\n",
    "\n",
    "3. **Ending Point (Upper-Right Corner):**\n",
    "   - **Threshold = 0.0**: The model predicts *everyone* will churn.\n",
    "   - **TPR = 100%**: All churning customers are found.\n",
    "   - **FPR = 100%**: All non-ch customersurning are wrongly labeled as churning.\n",
    "\n",
    "4. **What the Curve Shows:**\n",
    "   - Each point on the ROC curve represents a different threshold.\n",
    "   - A curve closer to the top-left corner (ideal) means the model works well.\n",
    "   - A curve close to the diagonal line (random baseline) means the model is no better than guessing.\n",
    "\n",
    "5. **AUC (Area Under the Curve):**\n",
    "   - AUC measures the overall performance of the model.\n",
    "   - Higher AUC = better at distinguishing churning vs. non-churning customers.\n",
    "\n",
    "In short, ROC curves help pick the best threshold to balance finding true churning customers (high TPR) while avoiding false alarms (low FPR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f8028",
   "metadata": {},
   "source": [
    "## 4.6 ROC AUC\n",
    "\n",
    "(source: https://knowmledge.com/2023/10/07/ml-zoomcamp-2023-evaluation-metrics-for-classification-part-6/)\n",
    "\n",
    "* Area under the ROC curve - useful metric\n",
    "* Interpretation of AUC"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c5b0cff-8cf8-43a8-8ff2-70645195b562",
   "metadata": {},
   "source": [
    "One way to quantify how close we are to the ideal point is by measuring the area under the ROC curve (AUC). AUC equals 0.5 for a random baseline and 1.0 for an ideal curve. Therefore, our model’s AUC should fall between 0.5 and 1.0. When AUC is less than 0.5, we’ve made a mistake. AUC = 0.8 is considered good, while 0.9 is great, but 0.6 is considered poor. We can calculate AUC using the scikit-learn package. This package is not specifically for roc curves, this is for any curve. It can calculate area under any curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58e54e-1f5b-483d-ad4d-602fe997638b",
   "metadata": {},
   "source": [
    "#### Useful metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509a4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643a6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# auc needs values for x-axis and y-axis\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c0c11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auc(df_scores.fpr, df_scores.tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774697c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auc(df_ideal.fpr, df_ideal.tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8658f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbdd64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There is a shortcut in scikit-learn package\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd31898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_val, y_pred)  # same result as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b061b-55b3-4f7c-aa98-599cf5747080",
   "metadata": {},
   "source": [
    "#### AUC interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb73e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neg = y_pred[y_val == 0]\n",
    "pos = y_pred[y_val == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb22b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847b909-5173-43f2-b139-77631b35eb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_ind = random.randint(0, len(pos) -1)\n",
    "neg_ind = random.randint(0, len(neg) -1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5333f9b-19f5-4c94-bae4-fa6d32d3c6bd",
   "metadata": {},
   "source": [
    "We want to compare the score of this positive example with the score of the negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4f031-e9f8-482f-9bed-571826e7a697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos[pos_ind] > neg[neg_ind]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc4be2a3-cd81-4c8e-b4c0-22e9ccdcc4ce",
   "metadata": {},
   "source": [
    "So, for this random example, this is true. We can do this 100,000 times and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7da8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "success = 0 \n",
    "\n",
    "for i in range(n):\n",
    "    pos_ind = random.randint(0, len(pos) - 1)\n",
    "    neg_ind = random.randint(0, len(neg) - 1)\n",
    "\n",
    "    if pos[pos_ind] > neg[neg_ind]:\n",
    "        success = success + 1\n",
    "\n",
    "success / n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcffbcca-510e-4a90-a17c-adb69a0b92dc",
   "metadata": {},
   "source": [
    "That result is quite close to roc_auc_score(y_val, y_pred) = 0.843850505725819."
   ]
  },
  {
   "cell_type": "raw",
   "id": "43a69a6a-5f8e-45d7-81cc-6c37116c1fad",
   "metadata": {},
   "source": [
    "Instead of implementing this manually, we can use NumPy. Be aware that in np.random.randint(low, high, size, dtype), ‘low’ is inclusive, and ‘high’ is exclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e19e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 50000\n",
    "\n",
    "np.random.seed(1)\n",
    "pos_ind = np.random.randint(0, len(pos), size=n)\n",
    "neg_ind = np.random.randint(0, len(neg), size=n)\n",
    "\n",
    "(pos[pos_ind] > neg[neg_ind]).mean()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a271c742-4387-4e1c-b34b-4c9fa1caafa1",
   "metadata": {},
   "source": [
    "Because of this interpretation, AUC is quite popular as a way of measuring the performance of binary classification models. It’s quite intuitive, and we can use it to assess how well our model ranks positive and negative examples and separates positive examples from negative ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336a636",
   "metadata": {},
   "source": [
    "## 4.7 Cross-Validation\n",
    "\n",
    "(source: https://knowmledge.com/2023/10/08/ml-zoomcamp-2023-evaluation-metrics-for-classification-part-7/)\n",
    "\n",
    "* Evaluating the same model on different subsets of data\n",
    "* Getting the average prediction and the spread within predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd853751-a4b8-45e6-8401-421e74b0db42",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluating the same model on different subsets of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e771d-f622-48ff-913b-0789f7f71adc",
   "metadata": {},
   "source": [
    "Parameter tuning means picking the best settings for a model. To do this, we split our data into three parts: training, validation, and testing. The validation set helps us find the best parameters. The test set is kept aside for now.\n",
    "\n",
    "We combine the training and validation sets into full_train and split it into three (k=3) smaller parts.\n",
    "\n",
    "1. Train on parts 1 & 2, validate on part 3, and calculate AUC.\n",
    "2. Train on parts 1 & 3, validate on part 2, and calculate AUC.\n",
    "3. Train on parts 2 & 3, validate on part 1, and calculate AUC.\n",
    "\n",
    "After this, we find the mean and standard deviation of the AUC values. The standard deviation tells us how stable the model is across different splits.\n",
    "\n",
    "This method is called K-Fold Cross-Validation, and it helps test a model on different parts of the dataset to ensure it's reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92708443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "\n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689044d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dv, model = train(df_train, y_train, C=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac302c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(df, dv, model):\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d88ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = predict(df_val, dv, model)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec3a4469-d26f-4cee-b0a9-a69728b60b15",
   "metadata": {},
   "source": [
    "We now have the ‘train’ and ‘predict’ functions in place. Let’s proceed to implement K-Fold Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7e607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2093e9-4867-419b-8186-cdbcddc78004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfold.split(df_full_train)\n",
    "# Output: <generator object _BaseKFold.split at 0x2838baf20>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f070a01-cf7d-4e8a-8c44-61d82efbffad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_idx, val_idx = next(kfold.split(df_full_train))\n",
    "len(train_idx), len(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f875867-96c8-4e0b-9931-69e191b051b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856bcf8-df41-437d-96d6-b64c6e1fc27c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can use iloc to select a part of this dataframe\n",
    "df_train = df_full_train.iloc[train_idx]\n",
    "df_val = df_full_train.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a53dcb05-b147-4924-815d-bc09e4d97736",
   "metadata": {},
   "source": [
    "The following code snippet demonstrates the implementation for 10 folds. Finally, we use the ‘roc_auc_score’ function to calculate and output the corresponding score for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20046f32-e33c-4c53-9f17-0c39f5cea448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    " \n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)  \n",
    "scores = []\n",
    " \n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    " \n",
    "    dv, model = train(df_train, y_train)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    " \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    " \n",
    "scores"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5870cfe2-d8fd-4cea-988d-3817b9c83210",
   "metadata": {},
   "source": [
    "Same implementation but this time with tqdm package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c8e07d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe7363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791123e4-1644-4948-baf3-88af854e6495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)  \n",
    "scores = []\n",
    " \n",
    "for train_idx, val_idx in tqdm(kfold.split(df_full_train)):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    " \n",
    "    dv, model = train(df_train, y_train)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    " \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b76e5-c2d0-44ad-ab64-3bd41bd2cea6",
   "metadata": {},
   "source": [
    "#### Getting the average prediction and the spread within predictions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69d2818f-f636-42d5-8627-9bee22599b06",
   "metadata": {},
   "source": [
    "We can utilize the scores generated to compute the average score across the 10 folds, which is 84.1%, with a standard deviation of 0.012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fab28-d177-4fd6-b9d5-ca679f3d4ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('%.3f +- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee035d3-e57c-4390-a242-6bad09bd4e52",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e353160-5dbf-4703-b354-f801e2396c26",
   "metadata": {},
   "source": [
    "We discussed parameter tuning, particularly the ‘C’ parameter in our LogisticRegression model, which serves as the regularization parameter with a default value of 1.0. We can include this ‘C’ parameter in our ‘train’ function. If ‘C’ is set to a very small value, it implies strong regularization. Additionally, we can address an annoying message by setting the ‘max_iter’ value to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a948a6-da04-4099-a077-a3504986ab07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    " \n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    " \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdfe9b-3ebc-4b75-a05e-e94528c3911b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dv, model = train(df_train, y_train, C=0.001)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63132b71-51c3-4067-9e91-d6988b054efd",
   "metadata": {},
   "source": [
    "We can iterate over various values for ‘C,’ keeping in mind that ‘C’ cannot be set to 0.0, as it would result in an ‘InvalidParameterError.’ The ‘C’ parameter for LogisticRegression must be a float within the range (0.0, inf], so we need to avoid using 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd99c9-bb3d-4ae8-bd9a-42daf4e941d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    " \n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)  \n",
    " \n",
    "for C in [0.001, 0.01, 0.1, 0.5, 1, 5, 10]:\n",
    "     \n",
    "    scores = []\n",
    " \n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "        y_train = df_train.churn.values\n",
    "        y_val = df_val.churn.values\n",
    " \n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    " \n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    " \n",
    "    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72b194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "raw",
   "id": "542ab87f-4ae2-403f-813b-8e9be5038cf0",
   "metadata": {},
   "source": [
    "We can implement the same procedure using the ‘tqdm’ package, which provides a more visually appealing output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce936aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "for C in tqdm([0.001, 0.01, 0.1, 0.5, 1, 5, 10]):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    "\n",
    "        y_train = df_train.churn.values\n",
    "        y_val = df_val.churn.values\n",
    "\n",
    "        dv, model = train(df_train, y_train, C=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "\n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    "\n",
    "    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93ea69b4-d5d4-4c66-a0df-b5b845e54bd7",
   "metadata": {},
   "source": [
    "Afterward, we aim to train our final model using the entire training dataset (df_full_train) and then validate it using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e81326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "968820ed-e23e-43e1-9b30-0d714055f68a",
   "metadata": {},
   "source": [
    "We observe that the AUC is slightly better than what we observed during k-fold cross-validation, though not significantly higher. This is expected when the difference is small."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f3f2e22-6bcd-475b-8410-a83ab38a67a3",
   "metadata": {},
   "source": [
    "In terms of when to use cross-validation versus traditional hold-out validation, for larger datasets, standard hold-out validation is often sufficient. However, if your dataset is smaller or you require insight into the model’s stability and variation across folds, then cross-validation is more appropriate. For larger datasets, consider using fewer splits (e.g., 2 or 3), while for smaller datasets, a higher number of splits (e.g., 10) may be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db242dff",
   "metadata": {},
   "source": [
    "## 4.8 Summary\n",
    "\n",
    "* Metric - a single number that describes the performance of a model\n",
    "* Accuracy - fraction of correct answers; sometimes misleading \n",
    "* Precision and recall are less misleading when we have class inbalance\n",
    "* ROC Curve - a way to evaluate the performance at all thresholds; okay to use with imbalance\n",
    "* K-Fold CV - more reliable estimate for performance (mean + std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941b0ca",
   "metadata": {},
   "source": [
    "## 4.9 Explore more\n",
    "\n",
    "* Check the precision and recall of the dummy classifier that always predict \"FALSE\"\n",
    "* F1 score = 2 * P * R / (P + R)\n",
    "* Evaluate precision and recall at different thresholds, plot P vs R - this way you'll get the precision/recall curve (similar to ROC curve)\n",
    "* Area under the PR curve is also a useful metric\n",
    "\n",
    "Other projects:\n",
    "\n",
    "* Calculate the metrics for datasets from the previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade60b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda-ml-zoomcamp:Python",
   "language": "python",
   "name": "conda-env-.conda-ml-zoomcamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
